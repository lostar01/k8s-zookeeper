# k8s-zookeeper: zookeeper cluster of DRAFT on Kubernetes
##### Steps:
* Build docker image via Quay.io
* Create kubernetes pods & services

        kubectl create -f zookeeper.yaml [--namespace=xxx]
* Teardown

        ./teardown.sh [--namespace=xxx]

-----
##### Notes:
* Enable zookeeper cluster nodes' intra communication via 3 services (1 service for 1 pod: zookeeper-1, zookeeper-2, and zookeeper-3)
* Use "zookeeper" service as a load balancer for clients. It doesn't matter on which zookeeper pod the clients are connected in a zookeeper cluster.
* Since pod name (== hostname), which is generated by k8s replication controller, won't be fixed, we use "service" name (e.g. zookeeper-1, zookeeper-2, zookeeper-3) rather than hostname in zoo.cfg.  
* Conceptually, the 3 services (zookeeper-1, zookeeper-2, zookeeper-3) could be headless services, but it doesn't work well in this case (unless delayed the startup of docker program within the pod, so that skydns can add IP entries for these just created pods)   
* Resilience verified OK: 
        
        (1) follower crash: container (docker stop), pod (kubectl stop), vm (reboot) 
        (2) leader crash: container (docker stop), pod (kubectl stop), vm (reboot)
* Cluster isolation verified OK:
 
        (1) namespaces under the same user context 
            [current-context: default]
            kubectl create -f zookeeper.yaml --namespace=default
            kubectl create -f zookeeper.yaml --namespace=staging
        (2) namespaces under different user context 
            kubectl config use-context develop
            [current-context: develop]
            kubectl create -f zookeeper.yaml --namespace=develop
        Fully isolated among above 3 namespaces

-----
##### TODO:
* Kubernetes 1.0.x doesn't support emptyDir volumes for containers running as non-root (it's commit in master branch, not v1.0.0 branch, refer to https://github.com/kubernetes/kubernetes/pull/9384 & https://github.com/kubernetes/kubernetes/issues/12627). Use root rather than zookeeper user instead at this moment.
* Try to spread zookeeper pods (created by different replication controllers, but using same service for external clients) on different nodes by experimenting kube-scheduler.service with policy configuration.
    https://github.com/kubernetes/kubernetes/blob/master/docs/admin/kube-scheduler.md
    https://docs.openshift.org/latest/admin_guide/scheduler.html#use-cases 
